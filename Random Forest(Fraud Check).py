# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KCcdpc_B5F_bzqcxTfJN7ZewHnFs1XMY
"""

from google.colab import files
uploaded=files.upload()

import pandas as pd
df=pd.read_csv("Fraud_check.csv")
df

# let's plot pair plot to visualise the attributes all at once
import seaborn as sns
sns.pairplot(data=df,hue ='Taxable.Income')

df.hist()

# Label encode
from sklearn.preprocessing import LabelEncoder 
LE = LabelEncoder()
df['Undergrad'] = LE.fit_transform(df['Undergrad'])
df['Marital.Status'] = LE.fit_transform(df['Marital.Status'])
df['Urban'] = LE.fit_transform(df['Urban'])

# Normalization function 
def norm_func(i):
    x = (i-i.min())/(i.max()-i.min())
    return (x)

# Normalized data frame (considering the numerical part of data)
df_norm = norm_func(df.iloc[:,0:])
df_norm.tail(10)

# Declaring features & target
X= df_norm.drop(['Taxable.Income'], axis=1)
Y= df_norm['Taxable.Income']

# Splitting data into train & test
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=10)

##Converting the Taxable income variable to bucketing. 
df_norm["income"]="<=30000"
df_norm.loc[df["Taxable.Income"]>=30000,"income"]="Good"
df_norm.loc[df["Taxable.Income"]<=30000,"income"]="Risky"

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
RFR = RandomForestRegressor(n_estimators=500,max_depth=8,max_features=0.7,
                      max_samples=0.6,random_state=10)

RFR.fit(X_train, Y_train)
Y_pred_train = RFR.predict(X_train) 
Y_pred_test = RFR.predict(X_test) 
Training_err = mean_squared_error(Y_train,Y_pred_train).round(2)
Test_err = mean_squared_error(Y_test,Y_pred_test).round(2)

print("Training_error: ",Training_err.round(2)*100)
print("Test_error: ",Test_err.round(2)*100)

from sklearn.ensemble import GradientBoostingRegressor
GBR = GradientBoostingRegressor(n_estimators=250,
                                max_depth=5,
                                max_features=0.7,
                                random_state=10,learning_rate=0.01)

GBR.fit(X_train, Y_train)
Y_pred_train = GBR.predict(X_train) 
Y_pred_test = GBR.predict(X_test) 
Training_err = mean_squared_error(Y_train,Y_pred_train).round(2)
Test_err = mean_squared_error(Y_test,Y_pred_test).round(2)

print("Training_error: ",Training_err.round(2)*100)
print("Test_error: ",Test_err.round(2)*100)

from sklearn.model_selection import GridSearchCV
import numpy as np

param_test1 = {'n_estimators':range(10,500,50),
               'learning_rate': np.arange(0.01,0.1,0.01),
               'max_features': np.arange(0.1,1,0.1)}

gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(), 
                        param_grid = param_test1, 
                        scoring='neg_mean_squared_error', cv=5)

gsearch1.fit(X,Y)

import numpy as np
np.sqrt(abs(gsearch1.best_score_))

gsearch1.best_params_